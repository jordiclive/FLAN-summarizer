import csv
import gc
import glob
import logging
import os
import re
import time
from argparse import Namespace
from pathlib import Path
from typing import Dict, List, Tuple

import configargparse
import nltk
import numpy as np
import pandas as pd
import torch
from data import CustomDataset
from lightning_base import BaseTransformer, add_generic_args, generic_train
from pytorch_lightning.utilities import rank_zero_info, rank_zero_only
from rouge_score import rouge_scorer, scoring
from torch.utils.data import DataLoader

logger = logging.getLogger(__name__)
ROUGE_KEYS = ["rouge1", "rouge2", "rougeL", "rougeLsum"]


def add_newline_to_end_of_each_sentence(x: str) -> str:
    """This was added to get rougeLsum scores matching published rougeL scores for BART and PEGASUS."""
    re.sub("<n>", "", x)  # remove pegasus newline char
    return "\n".join(nltk.sent_tokenize(x))


def calculate_rouge(
    pred_lns: List[str],
    tgt_lns: List[str],
    use_stemmer=True,
    rouge_keys=ROUGE_KEYS,
    return_precision_and_recall=False,
    bootstrap_aggregation=True,
    newline_sep=True,
) -> Dict:
    """Calculate rouge using rouge_scorer package.
    Args:
        pred_lns: list of summaries generated by model
        tgt_lns: list of groundtruth summaries (e.g. contents of val.target)
        use_stemmer:  Bool indicating whether Porter stemmer should be used to
        strip word suffixes to improve matching.
        rouge_keys:  which metrics to compute, defaults to rouge1, rouge2, rougeL, rougeLsum
        return_precision_and_recall: (False) whether to also return precision and recall.
        bootstrap_aggregation: whether to do the typical bootstrap resampling of scores. Defaults to True, if False
            this function returns a collections.defaultdict[metric: list of values for each observation for each subscore]``
        newline_sep:(default=True) whether to add newline between sentences. This is essential for calculation rougeL
        on multi sentence summaries (CNN/DM dataset).
    Returns:
         Dict[score: value] if aggregate else defaultdict(list) keyed by rouge_keys
    """
    scorer = rouge_scorer.RougeScorer(rouge_keys, use_stemmer=use_stemmer)
    aggregator = scoring.BootstrapAggregator()
    for pred, tgt in zip(tgt_lns, pred_lns):
        # rougeLsum expects "\n" separated sentences within a summary
        if newline_sep:
            pred = add_newline_to_end_of_each_sentence(pred)
            tgt = add_newline_to_end_of_each_sentence(tgt)
        scores = scorer.score(pred, tgt)
        aggregator.add_scores(scores)

    if bootstrap_aggregation:
        result = aggregator.aggregate()
        if return_precision_and_recall:
            return extract_rouge_mid_statistics(result)  # here we return dict
        else:
            return {k: round(v.mid.fmeasure * 100, 4) for k, v in result.items()}

    else:
        return aggregator._scores  # here we return defaultdict(list)


def extract_rouge_mid_statistics(dct):
    new_dict = {}
    for k1, v1 in dct.items():
        mid = v1.mid
        new_dict[k1] = {
            stat: round(getattr(mid, stat), 4)
            for stat in ["precision", "recall", "fmeasure"]
        }
    return new_dict


def label_smoothed_nll_loss(lprobs, target, epsilon, ignore_index=-100):
    """From fairseq"""
    if target.dim() == lprobs.dim() - 1:
        target = target.unsqueeze(-1)
    nll_loss = -lprobs.gather(dim=-1, index=target)
    smooth_loss = -lprobs.sum(dim=-1, keepdim=True)
    if ignore_index is not None:
        pad_mask = target.eq(ignore_index)
        nll_loss.masked_fill_(pad_mask, 0.0)
        smooth_loss.masked_fill_(pad_mask, 0.0)
    else:
        nll_loss = nll_loss.squeeze(-1)
        smooth_loss = smooth_loss.squeeze(-1)

    nll_loss = nll_loss.sum()  # mean()? Scared to break other math.
    smooth_loss = smooth_loss.sum()
    eps_i = epsilon / lprobs.size(-1)
    loss = (1.0 - epsilon) * nll_loss + eps_i * smooth_loss
    return loss, nll_loss


class ClassificationTransformer(BaseTransformer):
    def __init__(self, hparams):
        if type(hparams) == dict:
            hparams = Namespace(**hparams)

        super().__init__(hparams)

        self.val_losses = []
        self.fin_outputs = []
        self.fin_targets = []
        self.test_fin_outputs = []
        self.dataset_size = len(
            pd.read_json(
                Path(self.hparams.data_path).joinpath("train.json"), orient="split"
            )
        )
        self.save_path = ""
        self.model_name_or_path = self.hparams.model_name_or_path
        self.metric_names = ROUGE_KEYS
        self.decoder_start_token_id = None

        self.eval_beams = (
            self.model.config.num_beams
            if self.hparams.eval_beams is None
            else self.hparams.eval_beams
        )
        assert (
            self.eval_beams >= 1
        ), f"got self.eval_beams={self.eval_beams}. Need an integer > 1"

        if self.hparams.eval_max_gen_length is not None:
            self.eval_max_length = self.hparams.eval_max_gen_length
        else:
            self.eval_max_length = self.model.config.max_length
        self.val_metric = (
            self.default_val_metric
            if self.hparams.val_metric is None
            else self.hparams.val_metric
        )

        self.eval_min_length = self.hparams.eval_min_length
        rank_zero_info(
            "for decoding, eval_max_length={}, "
            "eval_min_length={}, eval_beams={}".format(
                self.eval_max_length, self.eval_min_length, self.eval_beams
            )
        )

    def ids_to_clean_text(self, generated_ids: List[int]):
        gen_text = self.tokenizer.batch_decode(
            generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True
        )
        return self.lmap(str.strip, gen_text)

    def forward(self, input_ids, **kwargs):
        return self.model(input_ids, **kwargs)

    def _step(self, batch: dict) -> Tuple:
        pad_token_id = self.tokenizer.pad_token_id
        src_ids, src_mask = batch["input_ids"], batch["attention_mask"]
        tgt_ids = batch["labels"]

        decoder_input_ids = self.model._shift_right(tgt_ids)

        outputs = self(
            src_ids,
            attention_mask=src_mask,
            decoder_input_ids=decoder_input_ids,
            use_cache=False,
        )

        lm_logits = outputs[0]
        if self.hparams.label_smoothing == 0:
            ce_loss_fct = torch.nn.CrossEntropyLoss(ignore_index=pad_token_id)

            loss = ce_loss_fct(
                lm_logits.view(-1, lm_logits.shape[-1]), tgt_ids.view(-1)
            )
        else:
            lprobs = torch.nn.functional.log_softmax(lm_logits, dim=-1)
            loss, nll_loss = label_smoothed_nll_loss(
                lprobs, tgt_ids, self.hparams.label_smoothing, ignore_index=pad_token_id
            )
        return (loss,)

    @property
    def pad(self) -> int:
        return self.tokenizer.pad_token_id

    def training_step(self, batch, batch_idx) -> Dict:

        loss = self._step(batch)[0]
        self.log(
            "train_loss",
            float(loss),
            on_step=True,
            on_epoch=True,
            prog_bar=True,
            logger=True,
            sync_dist=True,
        )

        logs = {"loss": loss}
        # tokens per batch
        logs["tpb"] = (
            batch["input_ids"].ne(self.pad).sum() + batch["labels"].ne(self.pad).sum()
        )
        logs["bs"] = batch["input_ids"].shape[0]
        logs["src_pad_tok"] = batch["input_ids"].eq(self.pad).sum()
        logs["src_pad_frac"] = batch["input_ids"].eq(self.pad).float().mean()

        # for k,v in logs.items():
        #     self.log(k,v,sync_dist=True)
        # self.log(
        #     "rate",
        #     self.trainer.lr_schedulers[0]["scheduler"].get_last_lr()[-1],
        #     rank_zero_only=True,
        # )
        return {"loss": loss}

    def get_dataloader(
        self, mode: str, batch_size: int, shuffle: bool = False
    ) -> DataLoader:
        "Load datasets. Called after prepare data."
        rank_zero_info(f"batch_size: {batch_size}")

        if mode == "dev":
            data = pd.read_json(
                Path(self.hparams.data_path).joinpath("val.json"), orient="split"
            )
            data = CustomDataset(
                data,
                self.tokenizer,
                input_length=self.hparams.max_seq_length,
                output_length=self.hparams.max_target_length,
            )

            val = DataLoader(
                data,
                batch_size=batch_size,
                shuffle=shuffle,
                num_workers=self.hparams.num_workers,
            )
            return val

        if mode == "train":
            data = pd.read_json(
                Path(self.hparams.data_path).joinpath("train.json"), orient="split"
            )
            data = CustomDataset(
                data,
                self.tokenizer,
                input_length=self.hparams.max_seq_length,
                output_length=self.hparams.max_target_length,
            )
            return DataLoader(
                data,
                batch_size=batch_size,
                shuffle=shuffle,
                num_workers=self.hparams.num_workers,
            )

    # @rank_zero_only
    def validation_step(self, batch, batch_idx):
        if self.hparams.skip_val:
            return 0
        if self.hparams.hf_checkpoint:
            save_path = Path(self.output_dir).joinpath("checkpoint-curr-best")
            self.model.save_pretrained(save_path)
            self.tokenizer.save_pretrained(save_path)
            raise ValueError("just saving")
        return self._generative_step(batch,batch_idx)

    def _generative_step(
        self, batch: dict, batch_idx=None, dataloader_idx=None
    ) -> dict:
        t0 = time.time()
        generated_ids = self.model.generate(
            batch["input_ids"],
            attention_mask=batch["attention_mask"],
            use_cache=True,
            length_penalty=self.hparams.length_penalty,
            decoder_start_token_id=self.decoder_start_token_id,
            num_beams=self.eval_beams,
            min_length=self.eval_min_length,
            max_length=self.eval_max_length,
            # no_repeat_ngram_size = self.hparams.no_repeat_ngram_size,
            # repetition_penalty = self.hparams.repetition_penalty,
        )
        gen_time = (time.time() - t0) / batch["input_ids"].shape[0]
        preds: List[str] = self.ids_to_clean_text(generated_ids)
        target: List[str] = self.ids_to_clean_text(batch["labels"])
        if batch_idx == 0:
            source: List[str] = self.ids_to_clean_text(batch["input_ids"])
            for i,j in enumerate(preds):
                rank_zero_info(f'source {source[i][:100]}...{source[i][-100:]} \n')
                rank_zero_info(f'pred: {preds[i]} \n target: {target[i]} \n\n\n')
        loss = self._step(batch)[0]
        base_metrics = {"loss": loss}
        rouge: Dict = self.calc_generative_metrics(preds, target)
        summ_len = np.mean(self.lmap(len, generated_ids))
        base_metrics.update(
            gen_time=gen_time, gen_len=summ_len, preds=preds, target=target, **rouge
        )
        self.log(
            "val_loss",
            loss,
            on_step=True,
            on_epoch=True,
            prog_bar=True,
            logger=True,
            sync_dist=True,
        )
        self.log(
            self.val_metric,
            rouge[self.val_metric],
            on_step=True,
            on_epoch=True,
            prog_bar=True,
            logger=True,
            sync_dist=True,
        )
        if dataloader_idx is not None:
            base_metrics.update(batch_idx=batch_idx, dataloader_idx=dataloader_idx)
        return base_metrics

    def calc_generative_metrics(self, preds, target) -> Dict:
        return calculate_rouge(preds, target)

    def lmap(self, f, x):
        """list(map(f, x))"""
        return list(map(f, x))

    def total_steps(self) -> int:
        """The number of total training steps that will be run. Used for lr scheduler purposes."""
        num_devices = max(1, self.hparams.gpus)

        effective_batch_size = (
            self.hparams.train_batch_size
            * self.hparams.accumulate_grad_batches
            * num_devices
        )

        return (self.dataset_size / effective_batch_size) * self.hparams.max_epochs

    @rank_zero_only
    def save_hf(self, path):

        rank_zero_info(path)
        save_path = Path(self.hparams.save_path).joinpath(path)
        self.save_path = save_path
        self.model.save_pretrained(save_path)
        self.tokenizer.save_pretrained(save_path)

    def validation_epoch_end(self, outputs: list) -> dict:
        if self.hparams.skip_val:
            return 0

        # generative_metrics = {
        #     k: np.array([x[k] for x in outputs]).mean()
        #     for k in self.metric_names + ["gen_time", "gen_len"]
        # }
        # metric_val = (
        #     generative_metrics[self.val_metric]
        # )
        #
        # rank_zero_info(f"Rouge2: {metric_val}")
        # self.log(
        #     "Rouge2",
        #     metric_val,
        #     rank_zero_only=True,
        # )
        gc.collect()
        torch.cuda.empty_cache()

    @staticmethod
    def add_model_specific_args(parser, root_dir):
        BaseTransformer.add_model_specific_args(parser, root_dir)
        parser.add_argument(
            "--max_seq_length",
            default=112,
            type=int,
            help="The maximum total input sequence length after tokenization. Sequences longer "
            "than this will be truncated, sequences shorter will be padded.",
        )

        parser.add_argument(
            "--m1_chip",
            default=False,
            type=bool,
            help="The number of GPUs allocated for this, it is by default 0 meaning none",
        )

        parser.add_argument(
            "--num_labels",
            default=256,
            type=int,
            help="The number of GPUs allocated for this, it is by default 0 meaning none",
        )

        parser.add_argument(
            "--gpus",
            default=0,
            type=int,
            help="The number of GPUs allocated for this, it is by default 0 meaning none",
        )

        parser.add_argument(
            "--overwrite_cache",
            action="store_true",
            help="Overwrite the cached training and evaluation sets",
        )
        parser.add_argument("--local", default=False, type=bool)
        parser.add_argument("--visible_devices", default="3", type=str)
        parser.add_argument("--offline", default=False, type=bool)
        parser.add_argument("--test_outputs", default="", type=str)

        parser.add_argument(
            "--max_source_length",
            default=512,  # 1024
            type=int,
            help="The maximum total input sequence length after tokenization. Sequences longer "
            "than this will be truncated, sequences shorter will be padded.",
        )
        parser.add_argument(
            "--max_target_length",
            default=60,  # 56
            type=int,
            help="The maximum total input sequence length after tokenization. Sequences longer "
            "than this will be truncated, sequences shorter will be padded.",
        )

        parser.add_argument(
            "--val_max_target_length",
            default=60,  # 142 # these defaults are optimized for CNNDM. For xsum, see README.md.
            type=int,
            help="The maximum total validation target length specified foor generation",
        )
        parser.add_argument(
            "--test_max_target_length",
            default=100,  # 142
            type=int,
            help="The maximum total test target length specified for generation",
        )

        parser.add_argument(
            "--n_train",
            type=int,
            default=-1,
            required=False,
            help="# examples. -1 means use all.",
        )
        parser.add_argument(
            "--n_val",
            type=int,
            default=-1,
            required=False,
            help="# examples. -1 means use all.",
        )
        parser.add_argument(
            "--n_test",
            type=int,
            default=-1,
            required=False,
            help="# examples. -1 means use all.",
        )

        parser.add_argument(
            "--label_smoothing", type=float, default=0.0, required=False
        )
        parser.add_argument("--eval_min_length", type=int, default=10, required=False)
        parser.add_argument("--skip_val", type=bool, default=False, required=False)

        parser.add_argument("--val_metric", type=str, default="rouge2", required=False)
        parser.add_argument(
            "--eval_max_gen_length",
            type=int,
            default=60,
            help="never generate more than n tokens",
        )
        parser.add_argument(
            "--length_penalty",
            type=float,
            default=1.0,
            help="length penalty specified for beam search",
        )

        parser.add_argument(
            "--T5_preamble",
            type=bool,
            default=False,
            required=False,
            help="Add the T5 preamble e.g. Summarize this text.",
        )
        parser.add_argument(
            "--no_repeat_ngram_size",
            type=float,
            default=3,
            help="length penalty specified for beam search",
        )

        return parser


def main():
    parser = configargparse.ArgumentParser(default_config_files=["config.yaml"])
    add_generic_args(parser, os.getcwd())
    parser = ClassificationTransformer.add_model_specific_args(parser, os.getcwd())
    args = parser.parse_args()

    if not os.path.exists(args.test_outputs):
        os.makedirs(args.test_outputs)

    if args.local:
        args.gpus = 0
    else:
        os.environ["CUDA_VISIBLE_DEVICES"] = args.visible_devices
    if args.offline or args.local:
        os.environ["WANDB_MODE"] = "offline"

    if args.output_dir is None:
        args.output_dir = os.path.join(
            "./results",
            f"{time.strftime('%Y%m%d_%H%M')}",
        )
        if not os.path.exists(args.output_dir):
            os.makedirs(args.output_dir)
    model = ClassificationTransformer(args)

    trainer = generic_train(model, args)

    if args.do_predict:
        checkpoints = list(
            sorted(
                glob.glob(
                    os.path.join(args.output_dir, "checkpoint-epoch=*.ckpt"),
                    recursive=True,
                )
            )
        )
        model = model.load_from_checkpoint(checkpoints[-1])
        return trainer.test(model)


if __name__ == "__main__":
    main()
